#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆAgent - æ·»åŠ è¯¦ç»†çš„Monitor RAGæ—¥å¿—è®°å½•
"""

import requests
from typing import Dict, List, Any
from openai import OpenAI
import re
import json
from termcolor import colored
import os
import sys
import time
import multiprocessing
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type
# from hipporag import HippoRAG  # æ³¨é‡Šæ‰ï¼Œä½¿ç”¨FAISSç‰ˆæœ¬
from copy import deepcopy
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('monitor_rag.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('MonitorRAG')

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, ".."))

from llm_agent.utils import LLMAgent
from configs.common_config import CommonConfig
from llm_agent.tools.tool_manager import ToolManager
from mcp_sandbox.MCP.rag_tool_emb_llm_judge import search_local_documents


class BaseAgent:
    def __init__(self, llm_config: Dict[str, Any]):
        self.llm_config: LLMAgent = LLMAgent(llm_config)
        self.rag_monitor = OpenAI(
            api_key=CommonConfig["OPENAI_CONFIG"]["authorization"],
            base_url=CommonConfig["OPENAI_CONFIG"]["url"]
        )
        self.rag_querier = OpenAI(
            api_key=CommonConfig["OPENAI_CONFIG"]["authorization"],
            base_url=CommonConfig["OPENAI_CONFIG"]["url"]
        )
        self.rag_injector = OpenAI(
            api_key=CommonConfig["OPENAI_CONFIG"]["authorization"],
            base_url=CommonConfig["OPENAI_CONFIG"]["url"]
        )
        
        # RAGå‚æ•°
        self.rag_chunk = 200  # ç›‘æ§çª—å£å¤§å°
        self.rag_overlapping = 50  # é‡å çª—å£å¤§å°
        self.max_rag = 3  # æœ€å¤§RAGä¸­æ–­æ¬¡æ•°
        
        logger.info(f"BaseAgent initialized with RAG parameters: chunk={self.rag_chunk}, overlapping={self.rag_overlapping}, max_rag={self.max_rag}")

    def check_rag(self, text: str):
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦RAGæ£€ç´¢
        """
        logger.info(f"ğŸ” [RAG Monitor] å¼€å§‹æ£€æŸ¥æ–‡æœ¬æ˜¯å¦éœ€è¦RAGæ£€ç´¢")
        logger.info(f"ğŸ“ [RAG Monitor] è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        logger.info(f"ğŸ“„ [RAG Monitor] è¾“å…¥æ–‡æœ¬é¢„è§ˆ: {text[:100]}...")
        
        prompt_1 = f"""
Analyze the following text and determine if responding to it accurately requires retrieving information from an external source.
If you find any doubt or uncertainty about a concept or term in the text, consider it necessary to rag. You should tend to use rag because it helps with reasoning.

If retrieval is required, answer: yes
If no retrieval is required, answer: no

Text:
{text}

Judgment:
"""
        try:
            logger.info(f"ğŸ¤– [RAG Monitor] è°ƒç”¨GPT-4.1-miniè¿›è¡ŒRAGåˆ¤æ–­")
            start_time = time.time()
            
            response = self.rag_monitor.chat.completions.create(
                model = 'gpt-4.1-mini',
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt_1}
                ],
                max_tokens=150,
                temperature=0
            )
            
            end_time = time.time()
            result = response.choices[0].message.content.strip()
            
            logger.info(f"â±ï¸ [RAG Monitor] APIè°ƒç”¨è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"ğŸ¯ [RAG Monitor] åˆ¤æ–­ç»“æœ: {result}")
            
        except Exception as e:
            logger.error(f"âŒ [RAG Monitor] APIè°ƒç”¨å¤±è´¥: {e}")
            return None

        if result.lower() != 'yes':
            logger.info(f"âœ… [RAG Monitor] åˆ¤æ–­ç»“æœ: ä¸éœ€è¦RAGæ£€ç´¢")
            return None

        logger.info(f"ğŸ”„ [RAG Monitor] éœ€è¦RAGæ£€ç´¢ï¼Œå¼€å§‹ç”ŸæˆæŸ¥è¯¢")
        
        prompt_2 = f"""
Your task is to generate a single, concise, and effective search query for retrieving the information required by the text below.

## Instructions
1. Return **only the search query** itself.
2. Do not include any explanations, punctuation, quotation marks, or other text.
3. The query should be direct and contain only the most essential keywords.

## Text
{text}

Search Query:
"""
        try:
            logger.info(f"ğŸ” [RAG Querier] è°ƒç”¨GPT-4.1-miniç”Ÿæˆæœç´¢æŸ¥è¯¢")
            start_time = time.time()
            
            response = self.rag_querier.chat.completions.create(
                model = 'gpt-4.1-mini',
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt_2}
                ],
                max_tokens=150,
                temperature=0
            )
            
            end_time = time.time()
            result = response.choices[0].message.content.strip()
            
            logger.info(f"â±ï¸ [RAG Querier] APIè°ƒç”¨è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"ğŸ¯ [RAG Querier] ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢: '{result}'")
            
            return result
        except Exception as e:
            logger.error(f"âŒ [RAG Querier] APIè°ƒç”¨å¤±è´¥: {e}")
            return None

    def rag_search(self, query: str) -> str:
        """
        æ‰§è¡ŒRAGæœç´¢
        """
        logger.info(f"ğŸ” [RAG Search] å¼€å§‹æœç´¢ï¼ŒæŸ¥è¯¢: '{query}'")
        start_time = time.time()
        
        try:
            docs = search_local_documents(query)
            end_time = time.time()
            
            logger.info(f"â±ï¸ [RAG Search] æœç´¢è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"ğŸ“Š [RAG Search] è¿”å›æ–‡æ¡£æ•°é‡: {len(docs)}")
            
            if self.llm_config.is_debug:
                logger.info(f"ğŸ“„ [RAG Search] æœç´¢ç»“æœé¢„è§ˆ:")
                for i, doc in enumerate(docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
                    logger.info(f"  [{i+1}] {doc[:100]}...")
            
            result = json.dumps(docs, indent=4)
            logger.info(f"âœ… [RAG Search] æœç´¢å®Œæˆï¼Œç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ [RAG Search] æœç´¢å¤±è´¥: {e}")
            return json.dumps([], indent=4)

    def add_rag(self, text: str, rag_result: str) -> str:
        """
        å°†RAGç»“æœé›†æˆåˆ°æ–‡æœ¬ä¸­
        """
        logger.info(f"ğŸ”„ [RAG Injector] å¼€å§‹é›†æˆRAGç»“æœ")
        logger.info(f"ğŸ“ [RAG Injector] åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        logger.info(f"ğŸ“„ [RAG Injector] RAGç»“æœé•¿åº¦: {len(rag_result)} å­—ç¬¦")
        
        prompt = f"""
You are an intelligent assistant that seamlessly integrates retrieved information into ongoing text generation. Your task is to naturally incorporate the retrieved information into the given text as if it were part of the original thought process.

## Instructions:
1. Integrate the retrieved information naturally into the text flow
2. Make it sound like the author just "thought of" or "remembered" this information
3. Use phrases like "I recall that...", "This reminds me that...", "I should also consider...", etc.
4. Maintain the original writing style and tone
5. Do not use explicit citations or references
6. Keep the integration smooth and conversational

## Original Text:
{text}

## Retrieved Information:
{rag_result}

## Enhanced Text:
"""
        try:
            logger.info(f"ğŸ¤– [RAG Injector] è°ƒç”¨GPT-4oè¿›è¡Œå†…å®¹é›†æˆ")
            start_time = time.time()
            
            response = self.rag_injector.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            end_time = time.time()
            result = response.choices[0].message.content.strip()
            
            logger.info(f"â±ï¸ [RAG Injector] APIè°ƒç”¨è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"ğŸ“ [RAG Injector] é›†æˆåæ–‡æœ¬é•¿åº¦: {len(result)} å­—ç¬¦")
            logger.info(f"âœ… [RAG Injector] å†…å®¹é›†æˆå®Œæˆ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ [RAG Injector] å†…å®¹é›†æˆå¤±è´¥: {e}")
            return text

    def call_api(self, user_prompt: str, enable_rag: bool = True):
        """
        è°ƒç”¨APIç”Ÿæˆå†…å®¹ï¼Œæ”¯æŒMonitor-based RAG
        """
        logger.info(f"ğŸš€ [API Call] å¼€å§‹APIè°ƒç”¨")
        logger.info(f"ğŸ“ [API Call] ç”¨æˆ·æç¤ºé•¿åº¦: {len(user_prompt)} å­—ç¬¦")
        logger.info(f"ğŸ”§ [API Call] RAGåŠŸèƒ½: {'å¯ç”¨' if enable_rag else 'ç¦ç”¨'}")
        
        if not enable_rag:
            logger.info(f"âš¡ [API Call] RAGå·²ç¦ç”¨ï¼Œç›´æ¥ç”Ÿæˆå†…å®¹")
            try:
                response = self.llm_config.call_model(user_prompt)
                logger.info(f"âœ… [API Call] å†…å®¹ç”Ÿæˆå®Œæˆ")
                return {"content": response, "type": "completed"}
            except Exception as e:
                logger.error(f"âŒ [API Call] å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
                return {"content": "", "type": "error", "error": str(e)}

        # Monitor-based RAGé€»è¾‘
        logger.info(f"ğŸ” [Monitor RAG] å¼€å§‹Monitor-based RAGæµç¨‹")
        
        rag_count = 0
        current_prompt = user_prompt
        rag_check_text = ""
        
        while rag_count < self.max_rag:
            logger.info(f"ğŸ”„ [Monitor RAG] ç¬¬ {rag_count + 1} æ¬¡ç”Ÿæˆå¾ªç¯")
            
            try:
                # æµå¼ç”Ÿæˆ
                logger.info(f"âš¡ [Monitor RAG] å¼€å§‹æµå¼ç”Ÿæˆ")
                response = self.llm_config.call_model_stream(current_prompt)
                
                generated_text = ""
                rag_check_text = ""
                
                for chunk in response:
                    if 'content' in chunk and chunk['content']:
                        generated_text += chunk['content']
                        rag_check_text += chunk['content']
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦RAG
                        if len(rag_check_text) >= self.rag_chunk:
                            logger.info(f"ğŸ¯ [Monitor RAG] è¾¾åˆ°RAGæ£€æŸ¥é˜ˆå€¼ ({self.rag_chunk} å­—ç¬¦)")
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦RAG
                            rag_query = self.check_rag(rag_check_text)
                            
                            if rag_query:
                                logger.info(f"ğŸ”„ [Monitor RAG] è§¦å‘RAGæ£€ç´¢ï¼ŒæŸ¥è¯¢: '{rag_query}'")
                                
                                # æ‰§è¡ŒRAGæœç´¢
                                rag_result = self.rag_search(rag_query)
                                
                                # é›†æˆRAGç»“æœ
                                enhanced_text = self.add_rag(rag_check_text, rag_result)
                                
                                # æ›´æ–°æç¤º
                                current_prompt = user_prompt + "\n\n" + enhanced_text
                                rag_count += 1
                                
                                logger.info(f"ğŸ”„ [Monitor RAG] RAGé›†æˆå®Œæˆï¼Œå¼€å§‹æ–°çš„ç”Ÿæˆå¾ªç¯")
                                logger.info(f"ğŸ“Š [Monitor RAG] å½“å‰RAGæ¬¡æ•°: {rag_count}/{self.max_rag}")
                                
                                # é‡ç½®æ£€æŸ¥æ–‡æœ¬ï¼Œä¿ç•™é‡å éƒ¨åˆ†
                                rag_check_text = rag_check_text[-self.rag_overlapping:]
                                break
                            else:
                                logger.info(f"âœ… [Monitor RAG] æ— éœ€RAGæ£€ç´¢ï¼Œç»§ç»­ç”Ÿæˆ")
                                # é‡ç½®æ£€æŸ¥æ–‡æœ¬ï¼Œä¿ç•™é‡å éƒ¨åˆ†
                                rag_check_text = rag_check_text[-self.rag_overlapping:]
                
                # å¦‚æœæ­£å¸¸å®Œæˆç”Ÿæˆ
                if not rag_query:
                    logger.info(f"âœ… [Monitor RAG] ç”Ÿæˆå®Œæˆï¼Œæ— RAGä¸­æ–­")
                    return {"content": generated_text, "type": "completed"}
                else:
                    logger.info(f"ğŸ”„ [Monitor RAG] å› RAGä¸­æ–­ï¼Œç»§ç»­ç”Ÿæˆ")
                    
            except Exception as e:
                logger.error(f"âŒ [Monitor RAG] ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                return {"content": "", "type": "error", "error": str(e)}
        
        logger.warning(f"âš ï¸ [Monitor RAG] è¾¾åˆ°æœ€å¤§RAGæ¬¡æ•°é™åˆ¶ ({self.max_rag})")
        return {"content": generated_text, "type": "max_rag_reached"}


class Eigen1Agent:
    def __init__(self, debug: bool = False, log_dir: str = "logs"):
        """
        åˆå§‹åŒ–Eigen1Agentï¼Œä½¿ç”¨o1-miniæ¨¡å‹
        """
        logger.info(f"ğŸš€ [Eigen1Agent] åˆå§‹åŒ–å¼€å§‹")
        logger.info(f"ğŸ”§ [Eigen1Agent] Debugæ¨¡å¼: {debug}")
        logger.info(f"ğŸ“ [Eigen1Agent] æ—¥å¿—ç›®å½•: {log_dir}")
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        
        # é…ç½®
        common_config = CommonConfig()
        
        # ä½¿ç”¨o1-miniæ¨¡å‹é…ç½®
        llm_config = {
            'model': 'o1-mini',
            'base_url': common_config["OPENAI_CONFIG"]["url"],
            'api_key': common_config["OPENAI_CONFIG"]["authorization"],
            'generation_config': {
                'max_tokens': 4000,
                'temperature': 0.7,
            },
            'stop_condition': r'<code[^>]*>((?:(?!<code).)*?)</code>',
            'tool_condition': r'<code[^>]*>((?:(?!<code).)*?)</code>',
            'is_debug': debug
        }
        
        self.deepseek_api_url = common_config['DEEPSEEK_CONFIG']['url']
        self.deepseek_api_key = common_config['DEEPSEEK_CONFIG']['authorization']
        self.sandbox_url = common_config['SANDBOX']['tool_link']
        
        self.chat_obj = LLMAgent(self.deepseek_api_url, self.deepseek_api_key, self.sandbox_url)
        
        logger.info(f"âœ… [Eigen1Agent] åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ¤– [Eigen1Agent] ä½¿ç”¨æ¨¡å‹: o1-mini")
        logger.info(f"ğŸ”— [Eigen1Agent] å·¥å…·æœåŠ¡å™¨: {self.sandbox_url}")

    def _forward_solver(self, query: str):
        """
        Solveræ­¥éª¤
        """
        logger.info(f"ğŸ§  [Solver] å¼€å§‹Solveræ­¥éª¤")
        logger.info(f"ğŸ“ [Solver] æŸ¥è¯¢: {query}")
        
        user_prompt = f"""
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†æé—®é¢˜ï¼š{query}

1. é¦–å…ˆè¿›è¡Œæœ¬åœ°æ–‡æ¡£æœç´¢ï¼ŒæŸ¥æ‰¾ç›¸å…³ä¿¡æ¯
2. è¯„ä¼°æœç´¢ç»“æœçš„è´¨é‡å’Œç›¸å…³æ€§
3. è¿›è¡Œç½‘ç»œæœç´¢éªŒè¯å’Œè¡¥å……ä¿¡æ¯
4. ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ

è¯·å¼€å§‹æ‰§è¡Œï¼š
"""
        
        try:
            result = self.chat_obj.call_model(user_prompt, assistant_prefix="æˆ‘å°†æŒ‰ç…§å·¥ä½œæµç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚")
            logger.info(f"âœ… [Solver] Solveræ­¥éª¤å®Œæˆ")
            logger.info(f"ğŸ“ [Solver] ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
            return result
        except Exception as e:
            logger.error(f"âŒ [Solver] Solveræ­¥éª¤å¤±è´¥: {e}")
            return str(e)

    def forward(self, query: str, question_id: str = "default"):
        """
        å®Œæ•´çš„Eigen1å·¥ä½œæµç¨‹
        """
        logger.info(f"ğŸ¯ [Eigen1Agent] å¼€å§‹å¤„ç†æŸ¥è¯¢")
        logger.info(f"ğŸ†” [Eigen1Agent] é—®é¢˜ID: {question_id}")
        logger.info(f"ğŸ“ [Eigen1Agent] æŸ¥è¯¢: {query}")
        
        # Step 1: Solver
        solver_result = self._forward_solver(query)
        
        logger.info(f"ğŸ‰ [Eigen1Agent] å¤„ç†å®Œæˆ")
        logger.info(f"ğŸ“Š [Eigen1Agent] æœ€ç»ˆç»“æœé•¿åº¦: {len(solver_result)} å­—ç¬¦")
        
        return solver_result


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•å¢å¼ºç‰ˆAgent")
    
    # åˆ›å»ºé…ç½®
    common_config = CommonConfig()
    llm_config = {
        'model': 'o1-mini',
        'base_url': common_config["OPENAI_CONFIG"]["url"],
        'api_key': common_config["OPENAI_CONFIG"]["authorization"],
        'generation_config': {
            'max_tokens': 4000,
            'temperature': 0.7,
        },
        'stop_condition': r'<code[^>]*>((?:(?!<code).)*?)</code>',
        'tool_condition': r'<code[^>]*>((?:(?!<code).)*?)</code>',
        'is_debug': True
    }
    
    # åˆ›å»ºBaseAgentå®ä¾‹
    agent = BaseAgent(llm_config)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = "åˆ†æä¸€ä¸‹äº¬ä¸œå’Œæ·˜å®çš„åŒºåˆ«"
    logger.info(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {test_query}")
    
    # æ‰§è¡ŒæŸ¥è¯¢
    result = agent.call_api(test_query, enable_rag=True)
    
    logger.info(f"âœ… æµ‹è¯•å®Œæˆï¼Œç»“æœç±»å‹: {result.get('type', 'unknown')}")
